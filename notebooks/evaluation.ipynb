{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b1213a8dbc01a2bbcdeb4e02d2d3f5411d6dec13eff8952fed902e4513bf8e24"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# WESAD Validation Notebook for FLIRT\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib; matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.autonotebook import trange\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import utils, model_selection, metrics\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import List\n",
    "import lightgbm as lgb\n",
    "import glob2\n",
    "import os \n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/home/fefespinola/ETHZ_Fall_2020/flirt-1')\n",
    "import flirt.simple"
   ]
  },
  {
   "source": [
    "The following function retrieves all HRV, EDA and ACC features per subject using the FLIRT pipeline\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-c9c2e5945b79>, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-c9c2e5945b79>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    The following function retrieves all HRV, EDA and ACC features per subject using the FLIRT pipeline\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_per_subject(path, window_length):\n",
    "    features = flirt.simple.get_features_for_empatica_archive(zip_file_path = path,\n",
    "                                      window_length = window_length,\n",
    "                                      window_step_size = 0.25,\n",
    "                                      hrv_features = False,\n",
    "                                      eda_features = True,\n",
    "                                      acc_features = False,\n",
    "                                      bvp_features = False,\n",
    "                                      temp_features = False,\n",
    "                                      debug = True)\n",
    "    return features"
   ]
  },
  {
   "source": [
    "The following function determines the time offsets of the start and end of each relevant analysis period (baseline, stress, amusement). These offsets are combined with the timestamp stating the start of recording, to determine the absolute timestamps of the sections of interest for each subject. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_label_timestamps(csv_path, StartingTime):\n",
    "\n",
    "    ID = csv_path.split('/', 3)[2]\n",
    "    df_timestamp = pd.read_csv(glob2.glob('project_data/WESAD/' + ID + '/*quest.csv')[0], delimiter = ';', header = 1).iloc[:2, :].dropna(axis = 1)\n",
    "    print('===================================')\n",
    "    print('Printing the timestamp for {0}'.format(ID))\n",
    "    print('===================================')\n",
    "    print(df_timestamp.head())\n",
    "    \n",
    "    # Start/End of experiment periods\n",
    "    print('\\nStart of the baseline: ' + str(df_timestamp['Base'][0]))\n",
    "    print('End of the baseline: ' + str(df_timestamp['Base'][1]))\n",
    "    print('Start of the fun: ' + str(df_timestamp['Fun'][0]))\n",
    "    print('End of the fun: ' + str(df_timestamp['Fun'][1]))\n",
    "    print('Start of the stress: ' + str(df_timestamp['TSST'][0]))\n",
    "    print('End of the stress: ' + str(df_timestamp['TSST'][1]))\n",
    "    \n",
    "    # Get start and end time and assign label into a dict\n",
    "    lab_dict = {'Base':0, 'TSST':1, 'Fun':2}\n",
    "    labels_times_dict = {}\n",
    "    for mode in df_timestamp.columns.tolist():\n",
    "        print('mode', mode)\n",
    "        if mode=='Base' or mode=='Fun' or mode=='TSST':\n",
    "            labels_times_dict[mode] = [StartingTime + timedelta(minutes = int(str(df_timestamp[mode][0]).split(\".\")[0]))+ timedelta                                         (seconds = int(str(df_timestamp[mode][0]).split(\".\")[1])), \n",
    "                                    StartingTime + timedelta(minutes = int(str(df_timestamp[mode][1]).split(\".\")[0])) + timedelta                                           (seconds = int(str(df_timestamp[mode][1]).split(\".\")[1])), lab_dict[mode]]\n",
    "            \n",
    "            #labels_times_dict[mode] = [StartingTime + timedelta(minutes = float(df_timestamp[mode][0])), \n",
    "                                  #StartingTime + timedelta(minutes = float(df_timestamp[mode][1])), lab_dict[mode]]\n",
    "        \n",
    "    return labels_times_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def find_label_start_time(csv_path):\n",
    "    ID = csv_path.split('/', 3)[2]\n",
    "    timestamp = open(glob2.glob('project_data/WESAD/' + ID + '/*respiban.txt')[0], \"r\")\n",
    "    for i in range(2):\n",
    "        line = (timestamp.readline())\n",
    "        line = line.strip()[2:]\n",
    "        if i==1:\n",
    "            dict = ast.literal_eval(line)\n",
    "            start_time_str = dict['00:07:80:D8:AB:58']['time']\n",
    "            date_str = dict['00:07:80:D8:AB:58']['date']\n",
    "            datetime_str = date_str + \" \" + start_time_str\n",
    "            #print(datetime_str)\n",
    "            date_time_obj = parse(datetime_str)\n",
    "            #print(date_time_obj)\n",
    "            start_time = date_time_obj\n",
    "            utc_time = start_time - timedelta(hours=2)\n",
    "\n",
    "    timestamp.close()\n",
    "\n",
    "    #df_timestamp = pd.read_table(glob2.glob('project_data/WESAD/' + ID + '/*respiban.txt')[0], delim_whitespace=True)#.iloc[:2, :].dropna(axis = 1)\n",
    "    print('===================================')\n",
    "    print('Printing the timestamp for {0}'.format(ID))\n",
    "    print('===================================')\n",
    "    #print(df_timestamp.head())\n",
    "    return utc_time"
   ]
  },
  {
   "source": [
    "Plots the training and validation classification metric evolution with the number of iterations. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===================================\nPrinting the timestamp for S10\n===================================\n2017-07-25 07:25:01\n===================================\nPrinting the timestamp for S11\n===================================\n2017-07-25 11:33:01\n===================================\nPrinting the timestamp for S13\n===================================\n2017-08-08 11:33:01\n===================================\nPrinting the timestamp for S14\n===================================\n2017-08-09 07:31:01\n===================================\nPrinting the timestamp for S15\n===================================\n2017-08-10 07:30:01\n===================================\nPrinting the timestamp for S16\n===================================\n2017-08-10 12:21:01\n===================================\nPrinting the timestamp for S17\n===================================\n2017-08-11 07:39:01\n===================================\nPrinting the timestamp for S2\n===================================\n2017-05-22 07:39:01\n===================================\nPrinting the timestamp for S3\n===================================\n2017-05-24 11:26:01\n===================================\nPrinting the timestamp for S4\n===================================\n2017-06-13 08:57:01\n===================================\nPrinting the timestamp for S5\n===================================\n2017-06-13 12:42:01\n===================================\nPrinting the timestamp for S6\n===================================\n2017-06-14 11:40:01\n===================================\nPrinting the timestamp for S7\n===================================\n2017-07-06 11:30:01\n===================================\nPrinting the timestamp for S8\n===================================\n2017-07-10 11:28:01\n===================================\nPrinting the timestamp for S9\n===================================\n2017-07-11 11:26:01\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/fefespinola/ETHZ_Fall_2020/') #local directory where the script is\n",
    "File_Path = glob2.glob('project_data/WESAD/**/*_readme.txt', recursive=True)\n",
    "for subject_path in File_Path:\n",
    "    start_time = find_label_start_time(subject_path)\n",
    "    print(start_time)"
   ]
  },
  {
   "source": [
    "def render_metric(eval_results, metric_name):\n",
    "    ax = lgb.plot_metric(evals_result, metric=metric_name, figsize=(10, 5))\n",
    "    #plt.show()\n",
    "    plt.savefig('/home/fefespinola/ETHZ_Fall_2020/plots/render_metric_all_ekf_feat.png')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "source": [
    "Plots the 10 top important classification features, i.e. the ones that influence the output the most."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_plot_importance(gbm, importance_type, max_features=10, ignore_zero=True, precision=3):\n",
    "    ax = lgb.plot_importance(gbm, importance_type=importance_type,\n",
    "                             max_num_features=max_features,\n",
    "                             ignore_zero=ignore_zero, figsize=(12, 8),\n",
    "                             precision=precision)\n",
    "    #plt.show()\n",
    "    plt.savefig('/home/fefespinola/ETHZ_Fall_2020/plots/feature_importance_all_ekf_feat.png')\n"
   ]
  },
  {
   "source": [
    "Main function that calls the above functions, determines the relevant data to use (i.e. that within the useful recording periods of baseline, stress and amusement) using the timestampp offsets, assignes the appropriate label to each sample and returns the full data with training samples and the corresponding labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    os.chdir('/home/fefespinola/ETHZ_Fall_2020/') #local directory where the script is\n",
    "    df_all = pd.DataFrame(None)\n",
    "    #relevant_features = pd.DataFrame(None)\n",
    "    File_Path = glob2.glob('project_data/WESAD/**/*_readme.txt', recursive=True)\n",
    "    window_length = 60 # in seconds\n",
    "    window_shift = 0.25 # in seconds\n",
    "    for subject_path in File_Path:\n",
    "        print(subject_path)\n",
    "        print(subject_path.split('/', 3)[2])\n",
    "        ID = subject_path.split('/', 3)[2]\n",
    "        zip_path = glob2.glob('project_data/WESAD/' + ID + '/*_Data.zip')[0]\n",
    "        print(zip_path)\n",
    "        features = get_features_per_subject(zip_path, window_length)\n",
    "        features.index.name = 'timedata'\n",
    "        E4Time = features.index[0]\n",
    "        print(E4Time)\n",
    "        StartingTime = find_label_start_time(subject_path)\n",
    "        print(StartingTime)\n",
    "        labels_times = find_label_timestamps(subject_path, StartingTime)\n",
    "        #features.index.tz_localize(tz='UTC')\n",
    "        relevant_features = features.loc[\n",
    "            ((features.index.tz_localize(tz=None) >= labels_times['Base'][0]) & (features.index.tz_localize(tz=None) <= labels_times['Base'][1])) \n",
    "            | ((features.index.tz_localize(tz=None) >= labels_times['Fun'][0]) & (features.index.tz_localize(tz=None) <= labels_times['Fun'][1])) \n",
    "            | ((features.index.tz_localize(tz=None) >= labels_times['TSST'][0]) & (features.index.tz_localize(tz=None) <= labels_times['TSST'][1]))]\n",
    "\n",
    "        relevant_features.insert(0, 'ID', ID)\n",
    "        relevant_features['label'] = np.zeros(len(relevant_features))\n",
    "        relevant_features.loc[(relevant_features.index.tz_localize(tz=None)>=labels_times['Fun'][0]) &\n",
    "                                (relevant_features.index.tz_localize(tz=None)<=labels_times['Fun'][1]), 'label'] = labels_times['Fun'][2]\n",
    "        relevant_features.loc[(relevant_features.index.tz_localize(tz=None)>=labels_times['TSST'][0]) & \n",
    "                            (relevant_features.index.tz_localize(tz=None)<=labels_times['TSST'][1]), 'label'] = labels_times['TSST'][2]\n",
    "\n",
    "        # concatenate all subjects and add IDs\n",
    "        df_all = pd.concat((df_all, relevant_features))\n",
    "    \n",
    "    print(df_all)\n",
    "\n",
    "    return df_all"
   ]
  },
  {
   "source": [
    "This function generates and saves feature matrices for the individual physiological signals "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_subset_features(df_all, feature_name: str, eda_method:str='lpf'):\n",
    "    \n",
    "    if feature_name=='physio':\n",
    "        small_df = df_all.loc[:, df_all.columns.str.startswith('hrv')&df_all.columns.str.startswith   ('eda')&df_all.columns.str.startswith('bvp')&df_all.columns.str.startswith('temp')]\n",
    "        filename = 'features_all_' + features_name +'_' + eda_method + '_feat.csv'\n",
    "    else:\n",
    "        small_df = df_all.loc[:, df_all.columns.str.startswith(feature_name)]\n",
    "        if feature_name=='eda':\n",
    "            filename = 'features_all_' + features_name +'_' + eda_method + '_feat.csv'\n",
    "        else:\n",
    "            filename = 'features_all_' + features_name + '_feat.csv'\n",
    "    small_df.to_csv(filename)\n"
   ]
  },
  {
   "source": [
    "The following function retrieves the correct training and testing data for LOSO cross-validation. It also deals with missing data (inf and nan), and scales the features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_train_valid_data(df_all, cv_subject):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    #training data\n",
    "    X_train = df_all.loc[df_all['ID']!=cv_subject]  # 500 entities, each contains 10 features\n",
    "    X_train = X_train.iloc[:, 1:len(df_all.columns)-1]\n",
    "    X_train = X_train.replace([np.nan, np.inf, -np.inf], -1000)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    y_train = df_all.loc[df_all['ID']!=cv_subject, ['label']]  # binary target\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    #validation data\n",
    "    X_test = df_all.loc[df_all['ID']==cv_subject]  # 500 entities, each contains 10 features\n",
    "    X_test = X_test.iloc[:, 1:len(df_all.columns)-1]\n",
    "    X_test = X_test.replace([np.nan, np.inf, -np.inf], -1000)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    y_test = df_all.loc[df_all['ID']==cv_subject, ['label']]   # binary target\n",
    "    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "    return X_train, y_train, train_data, X_test, y_test, test_data\n",
    " "
   ]
  },
  {
   "source": [
    "Run the evaluation script to retrieve the labeled data and train classifier to output f1-score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                   ID  eda_phasic_mean  eda_phasic_std  \\\n",
      "timedata                                                                 \n",
      "2017-07-25 07:27:06+00:00         S10         0.004517        0.003172   \n",
      "2017-07-25 07:27:06.250000+00:00  S10         0.004514        0.003174   \n",
      "2017-07-25 07:27:06.500000+00:00  S10         0.004511        0.003177   \n",
      "2017-07-25 07:27:06.750000+00:00  S10         0.004508        0.003180   \n",
      "2017-07-25 07:27:07+00:00         S10         0.004504        0.003183   \n",
      "...                               ...              ...             ...   \n",
      "2017-07-11 12:40:43+00:00          S9         0.003955        0.002862   \n",
      "2017-07-11 12:40:43.250000+00:00   S9         0.003955        0.002862   \n",
      "2017-07-11 12:40:43.500000+00:00   S9         0.003955        0.002862   \n",
      "2017-07-11 12:40:43.750000+00:00   S9         0.003955        0.002862   \n",
      "2017-07-11 12:40:44+00:00          S9         0.003955        0.002862   \n",
      "\n",
      "                                  eda_phasic_min  eda_phasic_max  \\\n",
      "timedata                                                           \n",
      "2017-07-25 07:27:06+00:00               0.000108        0.011479   \n",
      "2017-07-25 07:27:06.250000+00:00        0.000108        0.011479   \n",
      "2017-07-25 07:27:06.500000+00:00        0.000108        0.011479   \n",
      "2017-07-25 07:27:06.750000+00:00        0.000108        0.011479   \n",
      "2017-07-25 07:27:07+00:00               0.000108        0.011479   \n",
      "...                                          ...             ...   \n",
      "2017-07-11 12:40:43+00:00               0.000293        0.011234   \n",
      "2017-07-11 12:40:43.250000+00:00        0.000293        0.011234   \n",
      "2017-07-11 12:40:43.500000+00:00        0.000293        0.011234   \n",
      "2017-07-11 12:40:43.750000+00:00        0.000293        0.011234   \n",
      "2017-07-11 12:40:44+00:00               0.000293        0.011234   \n",
      "\n",
      "                                  eda_phasic_ptp  eda_phasic_sum  \\\n",
      "timedata                                                           \n",
      "2017-07-25 07:27:06+00:00               0.011371        1.084014   \n",
      "2017-07-25 07:27:06.250000+00:00        0.011371        1.083305   \n",
      "2017-07-25 07:27:06.500000+00:00        0.011371        1.082602   \n",
      "2017-07-25 07:27:06.750000+00:00        0.011371        1.081868   \n",
      "2017-07-25 07:27:07+00:00               0.011371        1.080987   \n",
      "...                                          ...             ...   \n",
      "2017-07-11 12:40:43+00:00               0.010941        0.949094   \n",
      "2017-07-11 12:40:43.250000+00:00        0.010941        0.949208   \n",
      "2017-07-11 12:40:43.500000+00:00        0.010941        0.949254   \n",
      "2017-07-11 12:40:43.750000+00:00        0.010941        0.949258   \n",
      "2017-07-11 12:40:44+00:00               0.010941        0.949219   \n",
      "\n",
      "                                  eda_phasic_energy  eda_phasic_skewness  \\\n",
      "timedata                                                                   \n",
      "2017-07-25 07:27:06+00:00                  0.007311             0.793917   \n",
      "2017-07-25 07:27:06.250000+00:00           0.007308             0.792830   \n",
      "2017-07-25 07:27:06.500000+00:00           0.007306             0.791745   \n",
      "2017-07-25 07:27:06.750000+00:00           0.007303             0.790551   \n",
      "2017-07-25 07:27:07+00:00                  0.007300             0.788953   \n",
      "...                                             ...                  ...   \n",
      "2017-07-11 12:40:43+00:00                  0.005719             0.886317   \n",
      "2017-07-11 12:40:43.250000+00:00           0.005719             0.886009   \n",
      "2017-07-11 12:40:43.500000+00:00           0.005720             0.885891   \n",
      "2017-07-11 12:40:43.750000+00:00           0.005720             0.885881   \n",
      "2017-07-11 12:40:44+00:00                  0.005720             0.885974   \n",
      "\n",
      "                                  eda_phasic_kurtosis  ...  \\\n",
      "timedata                                               ...   \n",
      "2017-07-25 07:27:06+00:00                   -0.449978  ...   \n",
      "2017-07-25 07:27:06.250000+00:00            -0.452758  ...   \n",
      "2017-07-25 07:27:06.500000+00:00            -0.455511  ...   \n",
      "2017-07-25 07:27:06.750000+00:00            -0.458384  ...   \n",
      "2017-07-25 07:27:07+00:00                   -0.461826  ...   \n",
      "...                                               ...  ...   \n",
      "2017-07-11 12:40:43+00:00                    0.060854  ...   \n",
      "2017-07-11 12:40:43.250000+00:00             0.060894  ...   \n",
      "2017-07-11 12:40:43.500000+00:00             0.060919  ...   \n",
      "2017-07-11 12:40:43.750000+00:00             0.060922  ...   \n",
      "2017-07-11 12:40:44+00:00                    0.060881  ...   \n",
      "\n",
      "                                  eda_tonic_fd_Power_0.45  \\\n",
      "timedata                                                    \n",
      "2017-07-25 07:27:06+00:00                    7.957927e-13   \n",
      "2017-07-25 07:27:06.250000+00:00             7.730439e-13   \n",
      "2017-07-25 07:27:06.500000+00:00             7.341431e-13   \n",
      "2017-07-25 07:27:06.750000+00:00             6.782175e-13   \n",
      "2017-07-25 07:27:07+00:00                    6.154202e-13   \n",
      "...                                                   ...   \n",
      "2017-07-11 12:40:43+00:00                    2.762906e-12   \n",
      "2017-07-11 12:40:43.250000+00:00             2.934920e-12   \n",
      "2017-07-11 12:40:43.500000+00:00             3.094471e-12   \n",
      "2017-07-11 12:40:43.750000+00:00             3.234396e-12   \n",
      "2017-07-11 12:40:44+00:00                    3.357786e-12   \n",
      "\n",
      "                                  eda_tonic_fd_kurtosis  eda_tonic_fd_iqr  \\\n",
      "timedata                                                                    \n",
      "2017-07-25 07:27:06+00:00                    234.718288        965.619651   \n",
      "2017-07-25 07:27:06.250000+00:00             234.718686        958.449460   \n",
      "2017-07-25 07:27:06.500000+00:00             234.719069        950.598131   \n",
      "2017-07-25 07:27:06.750000+00:00             234.719439        942.017157   \n",
      "2017-07-25 07:27:07+00:00                    234.719794        936.210140   \n",
      "...                                                 ...               ...   \n",
      "2017-07-11 12:40:43+00:00                    234.994344        173.898863   \n",
      "2017-07-11 12:40:43.250000+00:00             234.994278        174.728485   \n",
      "2017-07-11 12:40:43.500000+00:00             234.994211        176.252340   \n",
      "2017-07-11 12:40:43.750000+00:00             234.994142        177.732674   \n",
      "2017-07-11 12:40:44+00:00                    234.994072        178.248147   \n",
      "\n",
      "                                  eda_tonic_mfcc_mean  eda_tonic_mfcc_std  \\\n",
      "timedata                                                                    \n",
      "2017-07-25 07:27:06+00:00                    0.018481            0.415287   \n",
      "2017-07-25 07:27:06.250000+00:00             0.018481            0.417879   \n",
      "2017-07-25 07:27:06.500000+00:00             0.018481            0.420507   \n",
      "2017-07-25 07:27:06.750000+00:00             0.018481            0.423172   \n",
      "2017-07-25 07:27:07+00:00                    0.018481            0.425881   \n",
      "...                                               ...                 ...   \n",
      "2017-07-11 12:40:43+00:00                    0.018878            0.366513   \n",
      "2017-07-11 12:40:43.250000+00:00             0.018878            0.364890   \n",
      "2017-07-11 12:40:43.500000+00:00             0.018879            0.363314   \n",
      "2017-07-11 12:40:43.750000+00:00             0.018879            0.361766   \n",
      "2017-07-11 12:40:44+00:00                    0.018879            0.360228   \n",
      "\n",
      "                                  eda_tonic_mfcc_median  \\\n",
      "timedata                                                  \n",
      "2017-07-25 07:27:06+00:00                      0.024772   \n",
      "2017-07-25 07:27:06.250000+00:00               0.024345   \n",
      "2017-07-25 07:27:06.500000+00:00               0.024107   \n",
      "2017-07-25 07:27:06.750000+00:00               0.024084   \n",
      "2017-07-25 07:27:07+00:00                      0.024244   \n",
      "...                                                 ...   \n",
      "2017-07-11 12:40:43+00:00                      0.030316   \n",
      "2017-07-11 12:40:43.250000+00:00               0.030853   \n",
      "2017-07-11 12:40:43.500000+00:00               0.031541   \n",
      "2017-07-11 12:40:43.750000+00:00               0.031886   \n",
      "2017-07-11 12:40:44+00:00                      0.031933   \n",
      "\n",
      "                                  eda_tonic_mfcc_skewness  \\\n",
      "timedata                                                    \n",
      "2017-07-25 07:27:06+00:00                      -14.685793   \n",
      "2017-07-25 07:27:06.250000+00:00               -14.684440   \n",
      "2017-07-25 07:27:06.500000+00:00               -14.683455   \n",
      "2017-07-25 07:27:06.750000+00:00               -14.682846   \n",
      "2017-07-25 07:27:07+00:00                      -14.682608   \n",
      "...                                                   ...   \n",
      "2017-07-11 12:40:43+00:00                      -14.803054   \n",
      "2017-07-11 12:40:43.250000+00:00               -14.804696   \n",
      "2017-07-11 12:40:43.500000+00:00               -14.805968   \n",
      "2017-07-11 12:40:43.750000+00:00               -14.806450   \n",
      "2017-07-11 12:40:44+00:00                      -14.805785   \n",
      "\n",
      "                                  eda_tonic_mfcc_kurtosis  eda_tonic_mfcc_iqr  \\\n",
      "timedata                                                                        \n",
      "2017-07-25 07:27:06+00:00                      221.127789            0.026107   \n",
      "2017-07-25 07:27:06.250000+00:00               221.100372            0.025192   \n",
      "2017-07-25 07:27:06.500000+00:00               221.079658            0.024585   \n",
      "2017-07-25 07:27:06.750000+00:00               221.065841            0.024120   \n",
      "2017-07-25 07:27:07+00:00                      221.058834            0.023587   \n",
      "...                                                   ...                 ...   \n",
      "2017-07-11 12:40:43+00:00                      223.486484            0.016971   \n",
      "2017-07-11 12:40:43.250000+00:00               223.518040            0.018852   \n",
      "2017-07-11 12:40:43.500000+00:00               223.542473            0.020347   \n",
      "2017-07-11 12:40:43.750000+00:00               223.552377            0.021376   \n",
      "2017-07-11 12:40:44+00:00                      223.541673            0.021275   \n",
      "\n",
      "                                  label  \n",
      "timedata                                 \n",
      "2017-07-25 07:27:06+00:00           0.0  \n",
      "2017-07-25 07:27:06.250000+00:00    0.0  \n",
      "2017-07-25 07:27:06.500000+00:00    0.0  \n",
      "2017-07-25 07:27:06.750000+00:00    0.0  \n",
      "2017-07-25 07:27:07+00:00           0.0  \n",
      "...                                 ...  \n",
      "2017-07-11 12:40:43+00:00           2.0  \n",
      "2017-07-11 12:40:43.250000+00:00    2.0  \n",
      "2017-07-11 12:40:43.500000+00:00    2.0  \n",
      "2017-07-11 12:40:43.750000+00:00    2.0  \n",
      "2017-07-11 12:40:44+00:00           2.0  \n",
      "\n",
      "[135533 rows x 86 columns]\n",
      "['S10' 'S11' 'S13' 'S14' 'S15' 'S16' 'S17' 'S2' 'S3' 'S4' 'S5' 'S6' 'S7'\n",
      " 'S8' 'S9']\n",
      "---start classification---\n",
      "135533 86\n",
      "135533 86\n",
      "Columns dropped:  []\n",
      "running 15-fold CV...\n",
      "[[1386  229 3186]\n",
      " [ 798 1791  500]\n",
      " [ 224    0 1345]]\n",
      "[[3416 1278  107]\n",
      " [ 282 2514    5]\n",
      " [ 711  842    0]]\n",
      "[[2814 1987    0]\n",
      " [   0 2737    0]\n",
      " [ 618  991    0]]\n",
      "[[4801    0    0]\n",
      " [2749    0   32]\n",
      " [1569    0    0]]\n",
      "[[4105  341  263]\n",
      " [ 899 1846   80]\n",
      " [1139  430    0]]\n",
      "[[4801    0    0]\n",
      " [ 312 2565    4]\n",
      " [1228    0  145]]\n",
      "[[ 377  718 3710]\n",
      " [ 898 1847  228]\n",
      " [1014  137  526]]\n",
      "[[3971  685    1]\n",
      " [ 421 1831  181]\n",
      " [1497    0   32]]\n",
      "[[3151 1250  240]\n",
      " [ 676 1869   96]\n",
      " [1283   97   21]]\n",
      "[[4605    0    0]\n",
      " [ 271 2394   28]\n",
      " [1541    0   28]]\n",
      "[[2005 1469 1399]\n",
      " [ 189 2450   22]\n",
      " [1005  521   51]]\n",
      "[[4314  180  307]\n",
      " [ 896 1785    0]\n",
      " [ 951  618    0]]\n",
      "[[4432  228   93]\n",
      " [ 741 1344  556]\n",
      " [1249  320    0]]\n",
      "[[4640  117    0]\n",
      " [ 506 2147    0]\n",
      " [1453    0    0]]\n",
      "[[4566  137   98]\n",
      " [   0 2481    0]\n",
      " [1403    0  166]]\n",
      "0.48230940278255263\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    df_all = main()\n",
    "    df_all.to_csv('/home/fefespinola/ETHZ_Fall_2020/features_all_eda_lpf_1.csv')\n",
    "    df_all = pd.read_csv('/home/fefespinola/ETHZ_Fall_2020/features_all_eda_lpf_1.csv')\n",
    "    df_all.set_index('timedata', inplace=True)\n",
    "    #df_all = df_all.loc[:, ~df_all.columns.str.startswith('hrv')]\n",
    "    print(df_all)\n",
    "    ID=df_all.ID\n",
    "    print(ID.unique())\n",
    "\n",
    "    ### for binary classification uncomment line below \n",
    "    #df_all['label'].replace(2, 0, inplace=True)\n",
    "    #print('===== BINARY ====')\n",
    "    #print('binary----',df_all)\n",
    "    \n",
    "    print('---start classification---')\n",
    "\n",
    "    df_all.round(4)\n",
    "    df = df_all.replace([np.inf, -np.inf], np.nan) # np.inf leads to problems with some techniques\n",
    "\n",
    "    # Clean columns that contain a lot of nan values \n",
    "    print(len(df), len(df.columns))\n",
    "    df = df.dropna(axis=1, thresh=int(len(df)*0.8))\n",
    "    print(len(df), len(df.columns))\n",
    "    print('Columns dropped: ', df_all.drop(df.columns, axis=1).columns.values)\n",
    "\n",
    "    stats = []\n",
    "\n",
    "    cv = model_selection.LeaveOneGroupOut()\n",
    "\n",
    "    X = df.drop(columns=['label', 'ID'])\n",
    "    y = df['label'].astype('int')\n",
    "    groups = df['ID']\n",
    "    print(\"running %d-fold CV...\" % (cv.get_n_splits(X, y, groups)))\n",
    "\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "    for train_index, test_index in cv.split(X, y, groups):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        #### for binary classification uncomment line below\n",
    "        #params = {'objective': 'binary', 'is_unbalance': True}\n",
    "        params = {'objective': 'multiclass', 'metric': 'multi_logloss', 'num_class':3,  'is_unbalance': True}\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        stats.append({\n",
    "            'f1': metrics.f1_score(y_test, y_pred, average=\"macro\")})\n",
    "            \n",
    "        #print(metrics.classification_report(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    stats = pd.DataFrame(stats)\n",
    "    print(stats.f1.mean())\n",
    "\n",
    "    '''\n",
    "    #parameters\n",
    "    param = {'metric': 'auc_mu', 'learning_rate': 0.01, 'num_leaves': 31, 'is_unbalance':True,\n",
    "        'verbose': 1, 'objective':'multiclass', 'num_class':3, 'lambda_l1':0, 'force_col_wise':True}\n",
    "    \n",
    "    subjects = ['S2', 'S3', 'S4', 'S10', 'S11', 'S13', 'S14', 'S15', 'S16', 'S17']\n",
    "\n",
    "    ##### Start Classification\n",
    "    f1_tot = 0\n",
    "    f1_dict = {}\n",
    "    #data with validation set\n",
    "    for subj in subjects:\n",
    "        # get data\n",
    "        print('===Training for LOSO ', subj, '===')\n",
    "        _, _, train_data, X_test, y_test, test_data = __get_train_valid_data(df_all, subj)\n",
    "    \n",
    "        evals_result = {}  # to record eval results for plotting\n",
    "        \n",
    "        #train normally\n",
    "        bst_norm = lgb.train(param, train_data, num_boost_round=550, valid_sets=[train_data, test_data],                        evals_result=evals_result, verbose_eval=10)\n",
    "\n",
    "        best_iteration = np.argmax(evals_result['valid_1']['auc_mu'][2:]) + 1\n",
    "        print ('best iter', best_iteration)\n",
    "        y_pred = bst_norm.predict(X_test, num_iteration=best_iteration)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        f1_metric = f1_score(y_test, y_pred, average='macro')\n",
    "        f1_dict[subj] = f1_metric\n",
    "        print(f1_dict)\n",
    "        f1_tot = f1_tot + f1_metric\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "    f1_tot = f1_tot/len(subjects)\n",
    "    print(f1_tot)\n",
    "    #render_metric(evals_result, param['metric'])\n",
    "    #render_plot_importance(bst_norm, importance_type='split')\n",
    "    '''"
   ]
  },
  {
   "source": [
    "# Get relevant features\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}